{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stac_mjx \n",
    "from pathlib import Path\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slicing frames:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slicing frames: 100%|██████████| 1000/1000 [00:00<00:00, 19200.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frames [7000:8000] to '/root/vast/eric/CVAT_mouse_reach/triangulate_optimize/joshua_data/points3d_keewui_params_clipped.h5' under the dataset 'tracks'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Paths to original and clipped files\n",
    "original_file_path = \"/root/vast/eric/CVAT_mouse_reach/triangulate_optimize/joshua_data/points3d_keewui_params.h5\"\n",
    "clipped_file_path = \"/root/vast/eric/CVAT_mouse_reach/triangulate_optimize/joshua_data/points3d_keewui_params_clipped.h5\"\n",
    "\n",
    "# Define the frame range\n",
    "start_frame = 7000\n",
    "end_frame = 8000\n",
    "\n",
    "# Open the original HDF5 file and read the dataset\n",
    "with h5py.File(original_file_path, \"r\") as original_h5:\n",
    "    original_dataset = original_h5[\"tracks\"]\n",
    "    total_frames = end_frame - start_frame\n",
    "\n",
    "    # Allocate memory for the sliced frames\n",
    "    truncated_data = np.empty(\n",
    "        (total_frames,) + original_dataset.shape[1:],  # preserve all dims except the first\n",
    "        dtype=original_dataset.dtype\n",
    "    )\n",
    "\n",
    "    # Use tqdm to show progress as frames are copied\n",
    "    for i, frame_idx in enumerate(tqdm(range(start_frame, end_frame), desc=\"Slicing frames\")):\n",
    "        truncated_data[i] = original_dataset[frame_idx]\n",
    "\n",
    "    truncated_data = truncated_data.squeeze()\n",
    "\n",
    "# Write the truncated data to a new HDF5 file\n",
    "with h5py.File(clipped_file_path, \"w\") as clipped_h5:\n",
    "    clipped_h5.create_dataset(\"tracks\", data=truncated_data)\n",
    "\n",
    "print(f\"Saved frames [{start_frame}:{end_frame}] to '{clipped_file_path}' under the dataset 'tracks'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "# Paths to original and clipped files\n",
    "clipped_file_path = \"/root/vast/eric/CVAT_mouse_reach/triangulate_optimize/joshua_data/points3d_keewui_params_clipped.h5\"\n",
    "\n",
    "# Open the original HDF5 file and read the dataset\n",
    "with h5py.File(clipped_file_path, \"r\") as clipped_h5:\n",
    "    clipped_dataset = clipped_h5[\"tracks\"]\n",
    "\n",
    "    print(clipped_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from jax import numpy as jnp\n",
    "import yaml\n",
    "import scipy.io as spio\n",
    "import pickle\n",
    "from typing import Text, Union\n",
    "from pynwb import NWBHDF5IO\n",
    "from ndx_pose import PoseEstimationSeries, PoseEstimation\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from omegaconf import DictConfig\n",
    "import stac_mjx.io_dict_to_hdf5 as ioh5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cfg: DictConfig, base_path: Union[Path, None] = None):\n",
    "    \"\"\"Load mocap data based on file type.\n",
    "\n",
    "    Loads mocap file based on filetype, and returns the data flattened\n",
    "    for immediate consumption by stac_mjx algorithm.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): Configs.\n",
    "        base_path (Union[Path, None], optional): Base path for file paths in configs. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Mocap data flattened into an np array of shape [#frames, keypointXYZ],\n",
    "        where 'keypointXYZ' represents the flattened 3D keypoint components.\n",
    "        The data is also scaled by multiplication with \"MOCAP_SCALE_FACTOR\", e.g.\n",
    "        if the mocap data is in mm and the model is in meters, this should be\n",
    "        0.001.\n",
    "\n",
    "    Raises:\n",
    "        ValueError if an unsupported filetype is encountered.\n",
    "        ValueError if ordered list of keypoint names is missing or\n",
    "        does not match number of keypoints.\n",
    "    \"\"\"\n",
    "    if base_path is None:\n",
    "        base_path = Path.cwd()\n",
    "\n",
    "    file_path = base_path / cfg.stac.data_path\n",
    "    # using pathlib\n",
    "    if file_path.suffix == \".mat\":\n",
    "        label3d_path = cfg.model.get(\"KP_NAMES_LABEL3D_PATH\", None)\n",
    "        data, kp_names = load_dannce(str(file_path), names_filename=label3d_path)\n",
    "    elif file_path.suffix == \".nwb\":\n",
    "        data, kp_names = load_nwb(file_path)\n",
    "    elif file_path.suffix == \".h5\":\n",
    "        data, kp_names = load_h5(file_path)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unsupported file extension. Please provide a .nwb or .mat file.\"\n",
    "        )\n",
    "\n",
    "    kp_names = kp_names or cfg.model.KP_NAMES\n",
    "\n",
    "    if kp_names is None:\n",
    "        raise ValueError(\n",
    "            \"Keypoint names not provided. Please provide an ordered list of keypoint names \\\n",
    "            corresponding to the keypoint data order.\"\n",
    "        )\n",
    "\n",
    "    if len(kp_names) != data.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"Number of keypoint names ({len(kp_names)}) is not the same as the number of keypoints in data ({data.shape[1]})\"\n",
    "        )\n",
    "\n",
    "    model_inds = [\n",
    "        kp_names.index(src) for src, dst in cfg.model.KEYPOINT_MODEL_PAIRS.items()\n",
    "    ]\n",
    "\n",
    "    sorted_kp_names = [kp_names[i] for i in model_inds]\n",
    "\n",
    "    # Scale mocap data to match model\n",
    "    data = data * cfg.model.MOCAP_SCALE_FACTOR\n",
    "    # Sort in kp_names order\n",
    "    data = jnp.array(data[:, :, model_inds])\n",
    "    # Flatten data from [#num frames, #keypoints, xyz]\n",
    "    # into [#num frames, #keypointsXYZ]\n",
    "    data = jnp.transpose(data, (0, 2, 1))\n",
    "    data = jnp.reshape(data, (data.shape[0], -1))\n",
    "\n",
    "    return data, sorted_kp_names\n",
    "\n",
    "\n",
    "def load_dannce(filename, names_filename=None):\n",
    "    \"\"\"Load mocap data from .mat file.\n",
    "\n",
    "    .mat file is presumed to be constructed by dannce:\n",
    "    (https://github.com/spoonsso/dannce). In particular this means it relies on\n",
    "    the data being in millimeters [num frames, num keypoints, xyz], and that we\n",
    "    use the data stored in the \"pred\" key.\n",
    "    \"\"\"\n",
    "    node_names = None\n",
    "    if names_filename is not None:\n",
    "        mat = spio.loadmat(names_filename)\n",
    "        node_names = [item[0] for sublist in mat[\"joint_names\"] for item in sublist]\n",
    "\n",
    "    data = _check_keys(spio.loadmat(filename, struct_as_record=False, squeeze_me=True))[\n",
    "        \"pred\"\n",
    "    ]\n",
    "    return data, node_names\n",
    "\n",
    "\n",
    "def load_nwb(filename):\n",
    "    \"\"\"Load mocap data from .nwb file.\n",
    "\n",
    "    Data is presumed [num frames, num keypoints, xyz].\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with NWBHDF5IO(filename, mode=\"r\", load_namespaces=True) as io:\n",
    "        nwbfile = io.read()\n",
    "        pose_est = nwbfile.processing[\"behavior\"][\"PoseEstimation\"]\n",
    "        node_names = pose_est.nodes[:].tolist()\n",
    "        data = np.stack(\n",
    "            [pose_est[node_name].data[:] for node_name in node_names], axis=-1\n",
    "        )\n",
    "\n",
    "    return data, node_names\n",
    "\n",
    "\n",
    "def load_h5(filename):\n",
    "    \"\"\"Load .h5 file formatted as [frames, xyz, keypoints].\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the .h5 file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the data from the .h5 file.\n",
    "    \"\"\"\n",
    "    # TODO add track information\n",
    "    data = {}\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        for key in f.keys():\n",
    "            data[key] = f[key][()]\n",
    "\n",
    "    data = np.array(data[\"tracks\"])\n",
    "    data = np.squeeze(data, axis=1)\n",
    "    data = np.transpose(data, (0, 2, 1))\n",
    "    return data, None\n",
    "\n",
    "\n",
    "def _check_keys(dict):\n",
    "    \"\"\"Check if entries in dictionary are mat-objects.\n",
    "\n",
    "    Mat-objects are changed to nested dictionaries.\n",
    "    \"\"\"\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict\n",
    "\n",
    "\n",
    "def _todict(matobj):\n",
    "    \"\"\"A recursive function which constructs from matobjects nested dictionaries.\"\"\"\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "\n",
    "def _load_params(param_path):\n",
    "    \"\"\"Load parameters for the animal.\n",
    "\n",
    "    :param param_path: Path to .yaml file specifying animal parameters.\n",
    "    \"\"\"\n",
    "    with open(param_path, \"r\") as infile:\n",
    "        try:\n",
    "            params = yaml.safe_load(infile)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return params\n",
    "\n",
    "\n",
    "# FLY_MODEL: decide to keep or not!\n",
    "# def load_stac_ik_only(save_path):\n",
    "#     _, file_extension = os.path.splitext(save_path)\n",
    "#     if file_extension == \".p\":\n",
    "#         with open(save_path, \"rb\") as file:\n",
    "#             fit_data = pickle.load(file)\n",
    "#     elif file_extension == \".h5\":\n",
    "#         fit_data = ioh5.load(save_path)\n",
    "#     return fit_data\n",
    "\n",
    "\n",
    "def save(fit_data, save_path: Text):\n",
    "    \"\"\"Save data.\n",
    "\n",
    "    Save data as .p or .h5 file.\n",
    "\n",
    "    Args:\n",
    "        fit_data (numpy array): Data to write out.\n",
    "        save_path (Text): Path to save data. Defaults to None.\n",
    "    \"\"\"\n",
    "    if os.path.dirname(save_path) != \"\":\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    _, file_extension = os.path.splitext(save_path)\n",
    "    if file_extension == \".p\":\n",
    "        with open(save_path, \"wb\") as output_file:\n",
    "            pickle.dump(fit_data, output_file, protocol=2)\n",
    "    elif file_extension == \".h5\":\n",
    "        ioh5.save(save_path, fit_data)\n",
    "    else:\n",
    "        with open(save_path + \".p\", \"wb\") as output_file:\n",
    "            pickle.dump(fit_data, output_file, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m kp_data, sorted_kp_names \u001b[38;5;241m=\u001b[39m load_data(cfg, base_path)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Run stac\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m fit_path, ik_only_path \u001b[38;5;241m=\u001b[39m \u001b[43mstac_mjx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stac\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m \u001b[49m\u001b[43mkp_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m \u001b[49m\u001b[43msorted_kp_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vast/eric/stac-mjx/stac_mjx/main.py:64\u001b[0m, in \u001b[0;36mrun_stac\u001b[0;34m(cfg, kp_data, kp_names, base_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m ik_only_path \u001b[38;5;241m=\u001b[39m base_path \u001b[38;5;241m/\u001b[39m cfg\u001b[38;5;241m.\u001b[39mstac\u001b[38;5;241m.\u001b[39mik_only_path\n\u001b[1;32m     62\u001b[0m xml_path \u001b[38;5;241m=\u001b[39m base_path \u001b[38;5;241m/\u001b[39m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mMJCF_PATH\n\u001b[0;32m---> 64\u001b[0m stac \u001b[38;5;241m=\u001b[39m \u001b[43mStac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Run fit_offsets if not skipping\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mstac\u001b[38;5;241m.\u001b[39mskip_fit_offsets \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/vast/eric/stac-mjx/stac_mjx/stac.py:77\u001b[0m, in \u001b[0;36mStac.__init__\u001b[0;34m(self, xml_path, cfg, kp_names)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kp_names \u001b[38;5;241m=\u001b[39m kp_names\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m mjcf\u001b[38;5;241m.\u001b[39mfrom_path(xml_path)\n\u001b[1;32m     73\u001b[0m (\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mj_model,\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body_site_idxs,\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_regularized,\n\u001b[0;32m---> 77\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_body_sites\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mj_model\u001b[38;5;241m.\u001b[39mbody(i)\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mj_model\u001b[38;5;241m.\u001b[39mnbody)\n\u001b[1;32m     81\u001b[0m ]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROOT_OPTIMIZATION_KEYPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel:\n",
      "File \u001b[0;32m~/vast/eric/stac-mjx/stac_mjx/stac.py:148\u001b[0m, in \u001b[0;36mStac._create_body_sites\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m    146\u001b[0m     parent \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m, v)\n\u001b[1;32m    147\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mKEYPOINT_INITIAL_OFFSETS[key]\n\u001b[0;32m--> 148\u001b[0m     \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msite\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    150\u001b[0m         name\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msphere\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    152\u001b[0m         size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.005\u001b[39m],\n\u001b[1;32m    153\u001b[0m         rgba\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0 0 0 0.8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    154\u001b[0m         pos\u001b[38;5;241m=\u001b[39mpos,\n\u001b[1;32m    155\u001b[0m         group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m rescale\u001b[38;5;241m.\u001b[39mrescale_subtree(\n\u001b[1;32m    159\u001b[0m     root,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mSCALE_FACTOR,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mSCALE_FACTOR,\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    163\u001b[0m physics \u001b[38;5;241m=\u001b[39m mjcf\u001b[38;5;241m.\u001b[39mPhysics\u001b[38;5;241m.\u001b[39mfrom_mjcf_model(root)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "# Enable XLA flags if on GPU\n",
    "stac_mjx.enable_xla_flags()\n",
    "\n",
    "# Choose parent directory as base path for data files\n",
    "base_path = Path(\"/root/vast/eric/stac-mjx/\")\n",
    "\n",
    "# Load configs\n",
    "cfg = stac_mjx.load_configs(base_path / \"configs\")\n",
    "\n",
    "# Load data\n",
    "kp_data, sorted_kp_names = load_data(cfg, base_path)\n",
    "\n",
    "# Run stac\n",
    "fit_path, ik_only_path = stac_mjx.run_stac(\n",
    " cfg,\n",
    " kp_data, \n",
    " sorted_kp_names, \n",
    " base_path=base_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.36445197, 0.09739944, 0.0721727 , ..., 0.4157864 , 0.1145862 ,\n",
       "         0.06622083],\n",
       "        [0.364458  , 0.09761437, 0.07216524, ..., 0.4159316 , 0.11456176,\n",
       "         0.06614963],\n",
       "        [0.3647369 , 0.09788801, 0.07192241, ..., 0.41599098, 0.11467312,\n",
       "         0.06618948],\n",
       "        ...,\n",
       "        [0.2977303 , 0.04571613, 0.06430809, ..., 0.2858675 , 0.05222658,\n",
       "         0.01296798],\n",
       "        [0.29835916, 0.04511481, 0.06477104, ..., 0.28832692, 0.05729491,\n",
       "         0.01196769],\n",
       "        [0.29902112, 0.04431643, 0.0652778 , ..., 0.29104403, 0.06134205,\n",
       "         0.01082894]], dtype=float32),\n",
       " ['Elbow', 'Wrist', 'Shoulder'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp_data, sorted_kp_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stac-mjx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
