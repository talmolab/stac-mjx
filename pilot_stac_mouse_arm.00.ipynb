{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stac_mjx \n",
    "from pathlib import Path\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slicing frames:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slicing frames: 100%|██████████| 1000/1000 [00:00<00:00, 19200.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frames [7000:8000] to '/root/vast/eric/CVAT_mouse_reach/triangulate_optimize/joshua_data/points3d_keewui_params_clipped.h5' under the dataset 'tracks'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Paths to original and clipped files\n",
    "original_file_path = \"/root/vast/eric/CVAT_mouse_reach/triangulate_optimize/joshua_data/points3d_keewui_params.h5\"\n",
    "clipped_file_path = \"/root/vast/eric/CVAT_mouse_reach/triangulate_optimize/joshua_data/points3d_keewui_params_clipped.h5\"\n",
    "\n",
    "# Define the frame range\n",
    "start_frame = 7000\n",
    "end_frame = 8000\n",
    "\n",
    "# Open the original HDF5 file and read the dataset\n",
    "with h5py.File(original_file_path, \"r\") as original_h5:\n",
    "    original_dataset = original_h5[\"tracks\"]\n",
    "    total_frames = end_frame - start_frame\n",
    "\n",
    "    # Allocate memory for the sliced frames\n",
    "    truncated_data = np.empty(\n",
    "        (total_frames,) + original_dataset.shape[1:],  # preserve all dims except the first\n",
    "        dtype=original_dataset.dtype\n",
    "    )\n",
    "\n",
    "    # Use tqdm to show progress as frames are copied\n",
    "    for i, frame_idx in enumerate(tqdm(range(start_frame, end_frame), desc=\"Slicing frames\")):\n",
    "        truncated_data[i] = original_dataset[frame_idx]\n",
    "\n",
    "    truncated_data = truncated_data.squeeze()\n",
    "\n",
    "# Write the truncated data to a new HDF5 file\n",
    "with h5py.File(clipped_file_path, \"w\") as clipped_h5:\n",
    "    clipped_h5.create_dataset(\"tracks\", data=truncated_data)\n",
    "\n",
    "print(f\"Saved frames [{start_frame}:{end_frame}] to '{clipped_file_path}' under the dataset 'tracks'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "# Paths to original and clipped files\n",
    "clipped_file_path = \"/root/vast/eric/CVAT_mouse_reach/triangulate_optimize/joshua_data/points3d_keewui_params_clipped.h5\"\n",
    "\n",
    "# Open the original HDF5 file and read the dataset\n",
    "with h5py.File(clipped_file_path, \"r\") as clipped_h5:\n",
    "    clipped_dataset = clipped_h5[\"tracks\"]\n",
    "\n",
    "    print(clipped_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from jax import numpy as jnp\n",
    "import yaml\n",
    "import scipy.io as spio\n",
    "import pickle\n",
    "from typing import Text, Union\n",
    "from pynwb import NWBHDF5IO\n",
    "from ndx_pose import PoseEstimationSeries, PoseEstimation\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from omegaconf import DictConfig\n",
    "import stac_mjx.io_dict_to_hdf5 as ioh5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cfg: DictConfig, base_path: Union[Path, None] = None):\n",
    "    \"\"\"Load mocap data based on file type.\n",
    "\n",
    "    Loads mocap file based on filetype, and returns the data flattened\n",
    "    for immediate consumption by stac_mjx algorithm.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): Configs.\n",
    "        base_path (Union[Path, None], optional): Base path for file paths in configs. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Mocap data flattened into an np array of shape [#frames, keypointXYZ],\n",
    "        where 'keypointXYZ' represents the flattened 3D keypoint components.\n",
    "        The data is also scaled by multiplication with \"MOCAP_SCALE_FACTOR\", e.g.\n",
    "        if the mocap data is in mm and the model is in meters, this should be\n",
    "        0.001.\n",
    "\n",
    "    Raises:\n",
    "        ValueError if an unsupported filetype is encountered.\n",
    "        ValueError if ordered list of keypoint names is missing or\n",
    "        does not match number of keypoints.\n",
    "    \"\"\"\n",
    "    if base_path is None:\n",
    "        base_path = Path.cwd()\n",
    "\n",
    "    file_path = base_path / cfg.stac.data_path\n",
    "    # using pathlib\n",
    "    if file_path.suffix == \".mat\":\n",
    "        label3d_path = cfg.model.get(\"KP_NAMES_LABEL3D_PATH\", None)\n",
    "        data, kp_names = load_dannce(str(file_path), names_filename=label3d_path)\n",
    "    elif file_path.suffix == \".nwb\":\n",
    "        data, kp_names = load_nwb(file_path)\n",
    "    elif file_path.suffix == \".h5\":\n",
    "        data, kp_names = load_h5(file_path)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unsupported file extension. Please provide a .nwb or .mat file.\"\n",
    "        )\n",
    "\n",
    "    kp_names = kp_names or cfg.model.KP_NAMES\n",
    "\n",
    "    if kp_names is None:\n",
    "        raise ValueError(\n",
    "            \"Keypoint names not provided. Please provide an ordered list of keypoint names \\\n",
    "            corresponding to the keypoint data order.\"\n",
    "        )\n",
    "\n",
    "    if len(kp_names) != data.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"Number of keypoint names ({len(kp_names)}) is not the same as the number of keypoints in data ({data.shape[1]})\"\n",
    "        )\n",
    "\n",
    "    model_inds = [\n",
    "        kp_names.index(src) for src, dst in cfg.model.KEYPOINT_MODEL_PAIRS.items()\n",
    "    ]\n",
    "\n",
    "    sorted_kp_names = [kp_names[i] for i in model_inds]\n",
    "\n",
    "    # Scale mocap data to match model\n",
    "    data = data * cfg.model.MOCAP_SCALE_FACTOR\n",
    "    # Sort in kp_names order\n",
    "    data = jnp.array(data[:, :, model_inds])\n",
    "    # Flatten data from [#num frames, #keypoints, xyz]\n",
    "    # into [#num frames, #keypointsXYZ]\n",
    "    data = jnp.transpose(data, (0, 2, 1))\n",
    "    data = jnp.reshape(data, (data.shape[0], -1))\n",
    "\n",
    "    return data, sorted_kp_names\n",
    "\n",
    "\n",
    "def load_dannce(filename, names_filename=None):\n",
    "    \"\"\"Load mocap data from .mat file.\n",
    "\n",
    "    .mat file is presumed to be constructed by dannce:\n",
    "    (https://github.com/spoonsso/dannce). In particular this means it relies on\n",
    "    the data being in millimeters [num frames, num keypoints, xyz], and that we\n",
    "    use the data stored in the \"pred\" key.\n",
    "    \"\"\"\n",
    "    node_names = None\n",
    "    if names_filename is not None:\n",
    "        mat = spio.loadmat(names_filename)\n",
    "        node_names = [item[0] for sublist in mat[\"joint_names\"] for item in sublist]\n",
    "\n",
    "    data = _check_keys(spio.loadmat(filename, struct_as_record=False, squeeze_me=True))[\n",
    "        \"pred\"\n",
    "    ]\n",
    "    return data, node_names\n",
    "\n",
    "\n",
    "def load_nwb(filename):\n",
    "    \"\"\"Load mocap data from .nwb file.\n",
    "\n",
    "    Data is presumed [num frames, num keypoints, xyz].\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with NWBHDF5IO(filename, mode=\"r\", load_namespaces=True) as io:\n",
    "        nwbfile = io.read()\n",
    "        pose_est = nwbfile.processing[\"behavior\"][\"PoseEstimation\"]\n",
    "        node_names = pose_est.nodes[:].tolist()\n",
    "        data = np.stack(\n",
    "            [pose_est[node_name].data[:] for node_name in node_names], axis=-1\n",
    "        )\n",
    "\n",
    "    return data, node_names\n",
    "\n",
    "\n",
    "def load_h5(filename):\n",
    "    \"\"\"Load .h5 file formatted as [frames, xyz, keypoints].\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the .h5 file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the data from the .h5 file.\n",
    "    \"\"\"\n",
    "    # TODO add track information\n",
    "    data = {}\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        for key in f.keys():\n",
    "            data[key] = f[key][()]\n",
    "\n",
    "    data = np.array(data[\"tracks\"])\n",
    "    data = np.squeeze(data, axis=1)\n",
    "    data = np.transpose(data, (0, 2, 1))\n",
    "    return data, None\n",
    "\n",
    "\n",
    "def _check_keys(dict):\n",
    "    \"\"\"Check if entries in dictionary are mat-objects.\n",
    "\n",
    "    Mat-objects are changed to nested dictionaries.\n",
    "    \"\"\"\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict\n",
    "\n",
    "\n",
    "def _todict(matobj):\n",
    "    \"\"\"A recursive function which constructs from matobjects nested dictionaries.\"\"\"\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "\n",
    "def _load_params(param_path):\n",
    "    \"\"\"Load parameters for the animal.\n",
    "\n",
    "    :param param_path: Path to .yaml file specifying animal parameters.\n",
    "    \"\"\"\n",
    "    with open(param_path, \"r\") as infile:\n",
    "        try:\n",
    "            params = yaml.safe_load(infile)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return params\n",
    "\n",
    "\n",
    "# FLY_MODEL: decide to keep or not!\n",
    "# def load_stac_ik_only(save_path):\n",
    "#     _, file_extension = os.path.splitext(save_path)\n",
    "#     if file_extension == \".p\":\n",
    "#         with open(save_path, \"rb\") as file:\n",
    "#             fit_data = pickle.load(file)\n",
    "#     elif file_extension == \".h5\":\n",
    "#         fit_data = ioh5.load(save_path)\n",
    "#     return fit_data\n",
    "\n",
    "\n",
    "def save(fit_data, save_path: Text):\n",
    "    \"\"\"Save data.\n",
    "\n",
    "    Save data as .p or .h5 file.\n",
    "\n",
    "    Args:\n",
    "        fit_data (numpy array): Data to write out.\n",
    "        save_path (Text): Path to save data. Defaults to None.\n",
    "    \"\"\"\n",
    "    if os.path.dirname(save_path) != \"\":\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    _, file_extension = os.path.splitext(save_path)\n",
    "    if file_extension == \".p\":\n",
    "        with open(save_path, \"wb\") as output_file:\n",
    "            pickle.dump(fit_data, output_file, protocol=2)\n",
    "    elif file_extension == \".h5\":\n",
    "        ioh5.save(save_path, fit_data)\n",
    "    else:\n",
    "        with open(save_path + \".p\", \"wb\") as output_file:\n",
    "            pickle.dump(fit_data, output_file, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration iteration: 1/6\n",
      "Pose Optimization:\n",
      "Pose Optimization done in 29.298856258392334\n",
      "Frame 1 done in 29.249473571777344 with a final error of 0.0\n",
      "Mean: 0.0\n",
      "Standard deviation: 0.0\n",
      "starting offset optimization\n",
      "Begining offset optimization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/stac-mjx-env/lib/python3.11/site-packages/jaxopt/_src/optax_wrapper.py:120: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(update_fun, params, updates)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final error of 0.0009889188222587109\n",
      "offset optimization finished in 9.90920090675354\n",
      "Calibration iteration: 2/6\n",
      "Pose Optimization:\n",
      "Pose Optimization done in 0.03760814666748047\n",
      "Frame 1 done in 0.03358817100524902 with a final error of 0.0\n",
      "Mean: 0.0\n",
      "Standard deviation: 0.0\n",
      "starting offset optimization\n",
      "Begining offset optimization:\n",
      "Final error of 0.0009778663516044617\n",
      "offset optimization finished in 7.165462255477905\n",
      "Calibration iteration: 3/6\n",
      "Pose Optimization:\n",
      "Pose Optimization done in 0.03855466842651367\n",
      "Frame 1 done in 0.03409934043884277 with a final error of 0.0\n",
      "Mean: 0.0\n",
      "Standard deviation: 0.0\n",
      "starting offset optimization\n",
      "Begining offset optimization:\n",
      "Final error of 0.0009818419348448515\n",
      "offset optimization finished in 0.056619882583618164\n",
      "Calibration iteration: 4/6\n",
      "Pose Optimization:\n",
      "Pose Optimization done in 0.03251075744628906\n",
      "Frame 1 done in 0.02886819839477539 with a final error of 0.0\n",
      "Mean: 0.0\n",
      "Standard deviation: 0.0\n",
      "starting offset optimization\n",
      "Begining offset optimization:\n",
      "Final error of 0.0009899588767439127\n",
      "offset optimization finished in 0.05265402793884277\n",
      "Calibration iteration: 5/6\n",
      "Pose Optimization:\n",
      "Pose Optimization done in 0.033557891845703125\n",
      "Frame 1 done in 0.029928207397460938 with a final error of 0.0\n",
      "Mean: 0.0\n",
      "Standard deviation: 0.0\n",
      "starting offset optimization\n",
      "Begining offset optimization:\n",
      "Final error of 0.0009748790762387216\n",
      "offset optimization finished in 0.051671743392944336\n",
      "Calibration iteration: 6/6\n",
      "Pose Optimization:\n",
      "Pose Optimization done in 0.03219747543334961\n",
      "Frame 1 done in 0.02854776382446289 with a final error of 0.0\n",
      "Mean: 0.0\n",
      "Standard deviation: 0.0\n",
      "starting offset optimization\n",
      "Begining offset optimization:\n",
      "Final error of 0.000993182067759335\n",
      "offset optimization finished in 0.05101919174194336\n",
      "Final pose optimization\n",
      "Pose Optimization:\n",
      "Pose Optimization done in 0.033539772033691406\n",
      "Frame 1 done in 0.029820919036865234 with a final error of 0.0\n",
      "Mean: 0.0\n",
      "Standard deviation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Enable XLA flags if on GPU\n",
    "stac_mjx.enable_xla_flags()\n",
    "\n",
    "# Choose parent directory as base path for data files\n",
    "base_path = Path(\"/root/vast/eric/stac-mjx/\")\n",
    "\n",
    "# Load configs\n",
    "cfg = stac_mjx.load_configs(base_path / \"configs\")\n",
    "\n",
    "# Load data\n",
    "kp_data, sorted_kp_names = load_data(cfg, base_path)\n",
    "\n",
    "# Run stac\n",
    "fit_path, ik_only_path = stac_mjx.run_stac(\n",
    " cfg,\n",
    " kp_data, \n",
    " sorted_kp_names, \n",
    " base_path=base_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/vast/eric/stac-mjx/demo_fit.p\n"
     ]
    }
   ],
   "source": [
    "print(fit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stac-mjx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
